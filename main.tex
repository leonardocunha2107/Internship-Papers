\documentclass{article}
% \usepackage[a4,  margin=4 cm]{geometry}


% \usepackage{natbib}
\input{preamble.tex}
\usepackage{float}
\usepackage{wrapfig}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{todonotes}

%\usepackage[demo]{graphicx}

\title{Only tails matter:
Average-Case Universality and Robustness in the Convex Regime}

\author{Leonardo Cunha
\and Gauthier Gidel \\
\and Fabian Pedregosa \\
\and Courtney Paquette \\
\and Damien Scieur}


\date{August 2021}

\begin{document}
\maketitle
\begin{abstract}
    Recent works have studied the average convergence properties of first-order optimization methods on distributions of quadratic problems. The average-case framework allows a more fine-grained and representative analysis of convergence than usual worst-case results, in exchange for a more precise hypothesis over the data generating process, namely assuming knowledge of the expected spectral distribution (e.s.d) of the random matrix associated with the problem. In this work, we show that a problem's asymptotic average complexity is determined by the concentration of eigenvalues near the edges of the e.s.d. We argue that having à priori information on this concentration is a more grounded assumption than complete knowledge of the e.s.d.,  and that basing our analysis on the approximate concentration is effectively a middle ground between the coarseness of the worst-case convergence and this more unrealistic hypothesis. We introduce the Generalized Chebyshev method, asymptotically optimal under a hypothesis on this concentration, and globally optimal when the e.s.d. follows  a Beta distribution. We compare its performance to classical optimization algorithms, such as Gradient Descent or Nesterov's scheme, and we show that, asymptotically, Nesterov's method is universally nearly-optimal in the average-case.
\end{abstract}
\section{Introduction}


The analysis of the average complexity of algorithms has a long story in computer science. Notably, the quicksort algorithm presents a worst-case complexity of $O(n^2)$, but it's average case complexity is $O(n\log n)$ which makes it  empirically very competitive despite it's worst-case complexity. Average case complexity also drives much of the decisions made in cryptography \citep{bogdanov2006average}.

Despite its relevance, average case analyses are difficult to extend to other algorithms, in part because of the intrinsic issue of defining what would be a typical a distribution over problem instances. Recently though, \cite{pedregosa2020acceleration}  derived a framework to systemically evaluate the complexity of first-order methods when applied on distributions of quadratic minimization problems. This is done by relating the average-case convergence rate to the \textit{expected spectral distribution} (e.s.d) of the objective function's Hessian, which is a well-studied object on random matrix theory. Having access to this object in practice, though, is  a much stronger hypothesis  when compared to the worst-case analysis that relies only on the values of the edges of this distribution. 


\cite{paquette2020halting}  extended the average-case framework by introducing a noisy generative model for the problems. They further derived the average complexity of the Nesterov Accelerated Method \cite{nesterov2003introductory} on a particular distribution and showed the strong concentration of the metrics around a limiting value as dimensions go to infinite. 


\cite{scieur2020universal} showed that for a strongly convex problem with eigenvalues supported on a contiguous interval, the optimal average case complexity converges asymptotically to the one given by the Polyak Heavy Ball method in the worst-case.  

% In this work, we approach the non-strongly convex case, with a focus on the smooth subcase. In this scenario optimization drastically slows down, as Gradient Descent presents worst-case convergence in $\Theta(\frac{1}{t})$ and Nesterov is $\Theta(\frac{1}{t^2})$, where $t$ is the number of iterations, which matches a lower bound on worst-case convergence up to a constant factor \citep{nesterov2003introductory}. In the strongly convex case, where both the worst-case and average case are asymptotically equal. However, little is known on their performance in the average-case for convex problems. 

\subsection{Current limitations of average-case analysis} 

When analysing the state of the art of average-case method on quadratics problem, we observe major limitations that we address in this paper. First, little is known about the convergence rate on \textbf{convex problems}. Also, optimal average-case algorithms require an \textbf{exact estimation of the e.s.d} to guarantee an optimal convergence rate, their convergence rate under inexact e.s.d. is not known. Finally, the \textbf{non-smooth} is also discussed in \citep{pedregosa2020acceleration}, but with little details.

\paragraph{Convex problems.} The minimization of non-strongly convex problems is drastically slower than their strongly convex counterpart, as Gradient Descent presents worst-case convergence in $\Theta(\frac{1}{t})$ and Nesterov is $\Theta(\frac{1}{t^2})$. In the strongly convex case, both the worst-case and average case are asymptotically equal. However, little is known on optimal average-case rates for convex problems, as well as the average-case complexity of classical methods such as gradient descent or Nesterov's method.



% (below) Write this as a need rather than "we did that"

\paragraph{Exact estimation of the e.s.d.} In \citep{pedregosa2020acceleration}, the theoretical study of optimal algorithms in the average-case requires an exact estimation of the e.s.d. of the problem class. Such estimation may be hard, nor impossible to obtain in practical scenarios. Despite showing good performance when the e.s.d. is estimated with empirical quantities, there is no theoretical guarantees on the performance of the method when the e.s.d. is poorly estimated. There is therefore a need to analyse the algorithm's performance under different notions of uncertainty on the spectrum. This allows a practitioner to chose the best algorithm for a practical problem, even with imperfect \textit{à priori} information. 

% We also address the gap between the coarseness of the eigenvalue support assumption commonly found on worst-case analysis and the too precise assumption of the exact e.s.d. of the problem. We introduce a middle ground by looking at the concentration of eigenvalues around the edges of the support, allowing for a \emph{robust} analysis of the average case convergence. That is, we compare the algorithm's performance under different notions of uncertainty on the spectrum. This allows us  to choose the best algorithm for a practical problem based in realistic, i.e. imperfect, \textit{à priori} information. 

\paragraph{Non-smooth.} \citet{pedregosa2020acceleration} briefly introduce average-case optimal rates on non-smooth problems, when the e.s.d. is the Laguerre distribution $e^{-\lambda}$. In this paper, we extend the analysis to the generalized Laguerre $\lambda^\alpha e^{\lambda},\, \alpha >-1$.

% We also address the non-smooth convex case, i.e. the scenario where the Hessian`s eigenvalues can lie anywhere on the positive real and we don`t have a Lipschitz gradient, and show that an algorithm can be arbitrarily fast, in average,  depending on the problem`s distribution.

%The paper is organized as follows, in section \ref{section: average case} we present the average case framework. In section 4, we  show that the main aspects of the convergence, which from an optimization point view are the asymptotic rates, are determined by a low dimensional characterization of the distribution. Parametrizing the distributions in terms of the \textbf{concentration} of the eigenvalues around the edges allows us to effectively determine the precise rates for all continuous distributions supported in an interval $]0,L[$, where $L$ is the largest possible eigenvalue. 

%Our theoretical rates match numerical simulations and we show that our analysis is representative of the performance of algorithms on real data.

%Finally we'll state our main results : the Nesterov accelerated method asymptotic rate is close to the optimal one for relatively high concentrations of the eigenspectra near $0$, only a $\log$ term under a standard hypothesis. This means the Nesterov method is a very robust choice on practical scenarios.

% Todo Damien: write a proposal for the contribution

\subsection{Contributions}

Our main contribution is a fine-grained analysis of the average-case complexity on convex quadratic problems: we show that a problem's complexity depends on the concentration of the eigenvalues of e.s.d. around the edges of their support. From this perspective, we propose a family of \textbf{optimal algorithms} in the average-case, analyse their \textbf{robustness}, and finally exhibits a \textbf{universality} result for Nesterov's method. More precisely,
\begin{itemize}[leftmargin=*]
    \item \textbf{(Optimal algorithms).} In Section \ref{section: methods}, we propose the Generalized Chebyshev Method (GCM, Algorithm \ref{algo: chebyshev}), a family of algorithms whose parameters depend on the concentration of the e.s.d. around the edges of their support. If the parameters of the GCM method are set properly, the algorithm converges at a optimal average-case rate (Theorem \ref{thm: optimality} for smooth problems, Theorem \ref{thm:laguerrerates} for non-smooth problems), a rate that we show is faster than worst-case optimal methods like Nesterov acceleration. We show these rates to be representative of the practical performance of the algorithms in Fig.~\ref{fig: last figure}, and retrieve the classical worst-case rates as limits of the average case (see Table~\ref{table: rates}).
    \item \textbf{(Robustness).} Developing an optimal algorithm requires the knowledge of the exact e.s.d. However, in a practical scenarios, we only have access to an \textit{approximation} of the e.s.d. In Theorem \ref{thm: jacobirates} in Section \ref{section: robust average} we analyze the rate of GCM in the presence such mismatch. We also analyze the optimal average-case rates of distributions representing the smooth convex, non-smooth convex, and strongly convex settings and compare them with the worst-case rates (Table~\ref{table: rates}).
    \item \textbf{(Universality).} Finally, in Theorem \ref{the: neste rates}, we analyse the asymptotic average-case convergence rate of Nesterov method. We show that its convergence rate is nearly optimal (up to a logarithmic factor) under some natural assumptions over the data, namely a concentration of eigenvalues around $0$ similar to the Marchenko-Pastur measure. This contributes to the theoretical understanding of the numerical efficiency of Nesterov's acceleration.
\end{itemize}
% We analyse the average-case rate of convergence of gradient descent (Proposition \ref{}).  


% \paragraph{Contributions} From a practical standpoint, we propose the Generalized Chebyshev method which outperforms  Nesterov and is competitive with Conjugate Gradient  on linear regression for MNIST and CIFAR-10 datasets. 

% From a theoretical standpoint, we show the celebrated Nesterov method, 
% % which is easier to implement and more numerically stable, 
% to be very close to the asymptotic optimum under some natural assumptions over the data, namely a concentration of eigenvalues around $0$ similar to the Marchenko-Pastur measure. This adds to the theoretical understanding of why Nesterov acceleration is such a practical method. 
% \todo[inline]{Expand on what is new in terms of contribution. Clarify what does being optimal mean.}

% We show in Section~\ref{section: robust average} that the average case asymptotic convergence of  Gradient Descent, Nesterov and Generalized Chebyshev only depends on the concentrations of the problem's eigenvalues near the edges. We show these rates to be representative of the practical performance of the algorithms in Fig.~\ref{fig: last figure}, and retrieve the classical worst-case rates as limits of the average case.

% Finally, we characterize the optimum average case convergence in the smooth convex, non-smooth convex, and strongly convex settings and compare them with the worst-case rates. We summarize our results in Table~\ref{table: rates}.

\begin{figure}[t]
\centering
\begin{minipage}{0.40\textwidth} \vspace{0 pt} %
\includegraphics[clip, trim=4cm 0cm 0cm 0cm,width = 1.0 \linewidth]{new_imgs/new_spectrum.pdf}

\begin{small}
\caption{\small 
Representation of different spectra with different concentrations of eigenvalues around the edges of the support. The average-case rates for non-strongly problems are determined by these concentrations }
\end{small}
\end{minipage}%
\hfill
\begin{minipage}{0.55 \textwidth} \vspace{0 cm } %
\begin{small}
\captionsetup{type=table} %% tell latex to change to table
    \begin{tabular}{c|c|c}
         Regime&Worst-case& Average-Case  \\
         \hline
         Strongly conv. & $\left(1-\Theta(\nicefrac{1}{\sqrt{\kappa}})\right)^t$ &
         $\left(1-\Theta(\nicefrac{1}{\sqrt{\kappa}})\right)^t$\\ 
         \hline
         Smooth conv. & $\nicefrac{1}{t^2}$ & $\nicefrac{1}{t^{2\xi+4}}$\\
         \hline
          Convex& $\nicefrac{1}{\sqrt{t}}$ & $\nicefrac{1}{t^{\alpha+2}}$
    \end{tabular}
    \vspace{ 0.5 cm}
    %\\ \\ \\ \\ \\ \\ \\
    \caption{Comparison between function value worst-case and average case convergence. $\kappa$ is the condition number in the smooth strongly convex case. In the smooth convex case $\xi> -1$ is the concentration of eigenvalues around $0$ (see Assumption~\ref{assumption}) and in the non-smooth case we consider $d\mu\propto \lambda^\alpha e^{-x}$ }\label{table: rates}

\end{small}

\end{minipage}


\end{figure}
\section{Average-Case Analysis} \label{section: average case}


In this section, we recall the average-case analysis framework for random quadratic problems.
The main result is Theorem~\ref{thm: metrics}, which relates the expected error  to the \textit{expected spectral distribution} and the \textit{residual polynomial}. The one-to-one correspondence between the residual polynomials and first-order methods applied to quadratics will allow us  to pose the problem of finding an optimal method as a best approximation problem in the space of polynomials.


We  define a \textbf{random} quadratic problem:
\begin{problem}
Let $\HH \in \RR^{d \times d}$ be a random symmetric positive-definite matrix independent to $\xx^\star \in \RR^d$, a random vector that is the solution to the problem. We define the random quadratic minimization problem as
\begin{empheq}[box=\mybluebox]{equation*}\tag{OPT}\label{eq:quad_optim}
  \vphantom{\sum_0^i}\min_{\xx \in \RR^d} \Big\{ f(\xx) :=\!\mfrac{1}{2}(\xx\!-\!\xx^\star)^\top\!\HH(\xx\!-\!\xx^\star) \Big\}\,.
\end{empheq}
We are interested on minimizing the expected errors $\EE \|f(\xx_t) - f(\xx^\star)\|$, the expected function-value gap, and $\EE ||\nabla f(\xx_t)||^2$, the expected gradient norm, where $\xx_t$ is the $t$-th update of a first-order method starting from $\xx_0$ and $\EE$ is the expectation over the random variables $\HH, \xx_0$ and $\xx^\star$.
\end{problem}

The expectation we consider   is over the problem and not over any randomness of the algorithm.%, as would be common in the stochastic literature. In this paper we will only consider deterministic algorithms.


In this paper, we consider the class of \emph{first-order methods} to minimize \eqref{eq:quad_optim}. Methods in this class construct the iterates $\xx_t$ as
\begin{equation} \label{eq:first_order_methods}
    \xx_{t} \in \xx_0 + \Span\{ \nabla f(\xx_0), \ldots, \nabla f(\xx_{t-1})  \}\, .
\end{equation}
That is, $\xx_t$ belongs to the span of previous gradients. This class of algorithms includes for instance gradient descent and momentum, but not quasi-Newton methods, since the preconditioner could allow the iterates to go outside of the span. Furthermore, we will only consider \emph{oblivious} methods, that is, methods in which the coefficients of the update are known in advance and do not depend on previous updates. This leaves out some methods such as conjugate gradient or methods with line-search.


\paragraph{From First-Order Method to Polynomials.}
There is an intimate link between first-order methods and polynomials that simplifies the analysis on quadratic objectives. The next proposition shows that, with this link, we can assign to each optimization method a polynomial that determines its convergence.
% The following proposition states this relationship which relates the error at iteration $t$ with the error at initialization and the residual polynomial.
Following \cite{fischer1996polynomial}, we will say a polynomial $P_t$ is \textit{residual} if $P_t(0)=1$.

\begin{restatable}{proposition}{linkalgopolynomial}
    \label{prop:link_algo_polynomial} \citep{hestenes1952methods}
    Let $\xx_t$ be generated by a first-order method. Then there exists a residual polynomial $P_t$ of degree $t$, that verifies
    \begin{equation}\label{eq:polynomial_iterates}
        \vphantom{\sum^n}\xx_{t}-\xx^\star = P_t(\HH)(\xx_0-\xx^\star)~.
    \end{equation}
\end{restatable}

\begin{remark} \label{rmk: momentum based}
If the first-order method is further a \textbf{momentum method}, i.e.
$$
    \xx_{t+1}=\xx_t+h_t\nabla f(\xx_t)+m_t(\xx_t-\xx_{t-1}) .
$$
We can determine the polynomials by the recurrence $P_0=1$ and
    \begin{equation*}
        P_{t+1}(\lambda)=P_t(\lambda)+h_t\lambda P_t(\lambda)+m_t(P_t(\lambda)-P_{t-1}(\lambda)) .
    \end{equation*}
We note that while most popular F.O.M's can be posed as a momentum method, the Nesterov method cannot.
\end{remark}






A convenient way to collect statistics on the spectrum of a matrix is through its \emph{empirical spectral distribution}.


\begin{definition}
(\textbf{Expected spectral distribution [e.s.d]}). 
Let $\HH$ be a random matrix with eigenvalues $\{\lambda_1, \ldots, \lambda_d\}$. The \textbf{empirical spectral distribution} of $\HH$, called ${\mu}_{\HH}$, is the probability measure
\begin{equation}\label{eq:wighted_spectral_density}
    \mu_{\HH} \defas \frac{1}{d}{\textstyle{\sum_{i=1}^d}} \delta_{\lambda_i},
\end{equation}
where $\delta_{\lambda_i}$ is the Dirac delta, a distribution equal to zero everywhere except at $\lambda_i$ and whose integral over the entire real line is equal to one.

Since $\HH$ is random, the empirical spectral distribution $\mu_\HH$ is a  random variable in the space of measures. Its expectation over $\HH$ is called the \textbf{expected spectral distribution} and we denote it
\begin{equation}
\mu := \EE_{\HH}[\mu_{\HH}]\,.
\end{equation}
\end{definition}

We can link the e.s.d. of $\HH$ to the convergence of a first-order method on the distribution of $\HH$. In the following we will consider $\xx_0-\xx^\star$ and $\HH$ to be independent, with $\xx_0-\xx^\star$ sampled isotropically. %This isotropic hypothesis is not necessary and we could derive a similar analysis for more general distribution  of $x_0-x^\star$.

\begin{restatable}{thm}{metrics} \label{thm: metrics}
Let $\xx_t$ be generated by a first-order method associated to the polynomial $P_t$, the measure $\mu$ the  e.s.d. of $H$, and $\mathbb{E}[(\xx_0-\xx^\star)(\xx_0-\xx^\star)^T]=R^2\textbf{I}$ for some constant $R$. Then we can write the convergence metrics at time step  $t$ as
\begin{equation}
\begin{split}
\label{eq:error_norm_x}
  \mathbb{E}[\|\xx_t-\xx^\star\|^2] = { R^2} \int {P_t^2(\lambda) d\mu(\lambda)}, \hspace{1 cm}
    &\mathbb{E}[f(\xx_t)-f(\xx^\star)]=R^2\int P_t^2(\lambda)\lambda d\mu(\lambda)\\
    \text{and} \hspace{1 cm} \mathbb{E}[||\nabla f(\xx_t)||^2_2]&=R^2\int P_t^2(\lambda)\lambda^2d\mu(\lambda) .
\end{split}
\end{equation}
\end{restatable}
This shows that polynomials are a powerful abstraction as they allows us to write all of our convergence metrics in terms of it . For simplicity, we set $R^2 = 1$ and we will refer directly to the polynomials associated to a given method. We will refer to objective $l$ as the one associated to the added $\lambda^l$ term, i.e. the function-value is objective $l=1$.

This framework is  linked to the field of \textbf{orthogonal polynomials} by the next proposition. We construct an optimal method w.r.t. a given distribution through a family  of orthogonal polynomials associated to it.


\begin{restatable}{prop}{optimality}\citep{pedregosa2020acceleration}
 \label{prop: optimality}
 Let $P_t^l$ be defined as
 \begin{equation}
     P_t^l:={\arg \min}_{P_t(0)=1} \int P_t^2(\lambda) \lambda^l d\nu(\lambda).
 \end{equation}
 Then $(P_t^l)$ is the family of residual orthogonal polynomials w.r.t. to $\lambda^{l+1}d\nu$.
\end{restatable}

This theorem further implies that the optimal first-order method is a momentum method as Favard's theorem \cite{marcellan2001favard} tells us the orthogonal polynomials w.r.t. a given distribution are related through a \textbf{three term recurrence},

\begin{equation}
    P_{t+1}(\lambda)=a_tP_t(\lambda)+b_t\lambda P_t(\lambda)+(1-a_t)P_{t-1}(\lambda).
\end{equation}
Following Remark \ref{rmk: momentum based}, the optimal method is derived from this recurrence as
\begin{equation}
    \xx_{t+1}=\xx_t+(a_t-1)(\xx_t-\xx_{t-1})+b_t\nabla f(\xx_t)\,.
\end{equation}



\section{Generalized Chebyshev Method} \label{section: methods}
Being able to write the rates in terms of the \textit{expected spectral distribution} ties the average case framework to the field of \textit{random matrix theory}. Indeed, because of results from this field, certain e.s.d's are considered more natural than others. Indeed, it can be shown that the same distribution arises when we take the gram matrix of random  centered i.i.d. features with variance $\sigma^2$: the \textbf{Marchenko Pastur} distribution. 

\begin{definition}
The  Marchenko Pastur distribution associated to the parameter  $r$ and with scale $\sigma^2$ is given by 
\begin{equation}
d\mu_{MP}(\lambda)=\frac{1}{2\pi\sigma^2}\frac{\sqrt{(\lambda^+-\lambda)(\lambda-\lambda^-)}}{r\lambda} ,
\end{equation}
with $\lambda^+=\sigma^2(1+\sqrt{r})^2$, $\lambda^-=\sigma^2\max(0,(1-\sqrt{r})^2)$.
\end{definition}
The Marchenko Pastur distribution $\mu_{MP}$ can be considered a natural first model for e.s.d's as it arises universally from matrices with i.i.d. entries there is no specific distribution of the matrix to be considered. When $r=1$, i.e. $n=d$, we have $d\mu_{MP}\propto \lambda^{-1/2}\sqrt{\lambda^+-\lambda}$. %Though practical e.s.d's do not take the exact shape of the MP distribution, the same concentration near $0$ is often verified.


\cite{pedregosa2020acceleration} first derived the optimal method w.r.t. $\mu_{MP}$, and \cite{paquette2020halting} derived Nesterov's rates under the distribution. As we are concerned with being robust, a natural step is to consider the Beta weights.
\begin{definition}
    The (generalized) Beta weights with parameters $\tau,\xi$ and scale $L$ are given by the (non-normalized) pdf
    \begin{equation}
        \dif\mu(\lambda) = \lambda^\xi(L-\lambda)^\tau.
    \end{equation}
\end{definition} 
This family of distribution generalize the MP distribution, and both have similar concentrations near $0$ when $\xi\approx -1/2$.


The optimal method w.r.t. $\mu$ and metric $l$ is associated to a shifted Jacobi polynomial $\tp_t^{\alpha,\beta}$ with $\beta=\xi+l+1, \alpha=\tau$. When $\alpha=\beta=-1/2$, we retrieve the \textit{Chebyshev Method} \citep{flanders1950numerical}. As such, we call methods of this form the \textit{Generalized Chebyshev Method} (GCM). 
\begin{algorithm}
\caption{GCM($\alpha,\beta$)}
\textbf{Inputs}: Initial vector $\xx_0$, function $f$, smoothness parameter estimate $L$  \\


$\xx_{-1}\gets \textbf{0},\delta_0\gets0$   \\
\For{$t=1,\ldots,T$}{
$a_t\gets-\frac{2\left(\beta^2+\alpha\beta+(2t+1)(\alpha+\beta)+2t^2+2\right)(2t+\alpha+\beta+1)}{
            2(t+1)(t+\alpha+\beta+1)(2t+\alpha+\beta)} $\\
$b_t\gets\frac{(2t+\alpha+\beta+1)(2t+\alpha+\beta+2)}{
            L(t+1)(t+\alpha+\beta+1)}$ \\
$\gamma_t\gets-\frac{(t+\alpha)(t+\beta)(2t+\alpha+\beta+2)}{
            (t+1)(t+\alpha+\beta+1)(2t+\alpha+\beta)}$\\
$\delta_t\gets\frac{1}{a_t+\gamma_t\delta_{t-1}}$\\
$\xx_t\gets\xx_{t-1}+(\delta_ta_t-1)(\xx_{t-1}-\xx_{t-2})+\delta_t b_t\nabla f(\xx_{t-1})$
} \label{algo: chebyshev}
\end{algorithm}

We  also consider the Laguerre method, which is optimal w.r.t. $d\mu(x)=\frac{x^\alpha e^{-x}}{\Gamma(\alpha+1)}$, taking $\alpha$ as a parameter. This method is proposed to optimize non-smooth functions.\\
Both these methods are generalizations of one that have been proposed in \cite{pedregosa2020acceleration}. We show that Algorithm \ref{algo: chebyshev} corresponded to polynomials $\tp_t^{\alpha,\beta}$ and derive the Laguerre method in appendix \ref{jacobi recurrence}.\\
\begin{remark}
The Generalized Chebyshev takes the largest eigenvalue $L$ as a parameter, but the rates we will show are robust to an \textit{overestimation} of $L$.
\end{remark}

\section{Robust Average Case Rates} \label{section: robust average}
We will state our assumption over the spectral distributions. It effectively allows us to parametrize all of our distributions of interest in a way that characterizes the asymptotic convergence, diving them into equivalence classes .

\begin{assumption}
We will write $\nu_{\tau,\xi}$ for a continuous distribution supported in $(0,L]$ s.t. $\nu_{\tau,\xi}'(x)>0$ for $x\in [0,L]$, $d\nu_{\tau,\xi}=\Theta( \lambda^\xi)$ near $0$ and $d\nu_{\tau,\xi}=\Theta( (L-\lambda)^\tau)$ near $L$. 
\label{assumption}
\end{assumption}

We argue this is a much milder assumption to be made than the exact spectral distribution and it  any distribution modeling a smooth convex problem can be identified to one of these classes. 


The $\xi$ works as a measure of how close we are to the worst-case scenario, as it approaches $-1$. Samples in finite dimension of distributions with high values of $\xi$ will work as strongly convex functions in practice.


We show that $\nu_{\tau,\xi}$ indeed behaves like an equivalence class when considering the asymptotics of the convergence of the methods: only the concentrations near the edge matter. We do this by singling out from each of these classes the beta distributions for which we can compute the rates, then show the rates to be the same inside $\nu_{\tau,\xi}$.



\begin{restatable}{theorem}{robustjacobi}\label{thm: jacobirates}
A Generalized Chebyshev Method with parameters $(\alpha,\beta)$ applied to a problem with e.s.d. as in assumption \ref{assumption} has rates
\begin{align}
\mathbb{E}[f(\xx_t)-f(\xx^\star)]&\sim L\cdot C^{\alpha,\beta}_{1,\nu}
    \left\{\begin{array}{ll}
    t^{-1-2\beta} &\mbox{if } 
		  \alpha<\tau+1/2 \text{ and } \beta <\xi+3/2\\
		  t^{-2(\xi+2)}\log t& \mbox{if } 
		  \alpha=\tau+1/2 \text{ and } \beta =\xi+3/2\\
		  t^{2(\max\{\alpha-\beta-\tau,-\xi-1\}-1)}& \mbox{if } 
		  \alpha>\tau+1/2 \text{ or } \beta >\xi+3/2
	\end{array}\right. ,\\
	\mathbb{E}[||\nabla f(\xx_t)||^2_2]&\sim L^2\cdot C^{\alpha,\beta}_{2,\nu}
        \left\{\begin{array}{ll}
    t^{-1-2\beta} &\mbox{if } 
		  \alpha<\tau+1/2 \text{ and } \beta <\xi+5/2\\
		  t^{-2(\xi+3)}\log t& \mbox{if } 
		  \alpha=\tau+1/2 \text{ and } \beta =\xi+5/2\\
		  t^{2(\max\{\alpha-\beta-\tau,-\xi-2\}-1)}& \mbox{if } 
		  \alpha>\tau+1/2 \text{ or } \beta >\xi+5/2
	\end{array}\right. ,
\end{align}
where $C^{\alpha,\beta}_\nu$ is a distribution dependent constant.
\end{restatable}

Theorem \ref{thm: optimality} shows that a proper choice of $\alpha,\beta$ can indeed make the Jacobi polynomial asymptotically optimal w.r.t. to any $\nu_{\tau,\xi}$. 

\begin{restatable}{theorem}{jacoptimal}\label{thm: optimality}
Let $\nu$ follow Assumption \ref{assumption}.
The optimal asymptotic rates for respectively $\mathbb{E}[f(\xx_t)-f(\xx^\star)]$ and $\mathbb{E}[||\nabla f(\xx_t)||^2_2]$ are attained by the GCM with parameters respectively $(\tau,\xi+2)$ and $(\tau,\xi+3)$, and read
\[
    \mathbb{E}[f(\xx_t)-f(\xx^\star)] = \Theta(t^{-2(\xi+2)}), \qquad \mathbb{E}[||\nabla f(\xx_t)||^2_2] = \Theta(t^{-2(\xi+3)}).
\]

\end{restatable}

\begin{wrapfigure}{r}{0.62\textwidth}
    \vspace{-.8cm}
    \centering
    %\includegraphics[width=10 cm]{imgs/diagram.PNG}
    \includegraphics[width=0.45\textwidth]{new_imgs/cmap.png}
    % \begin{small}
    \caption{
    \small Illustration of the robustness of the Generalized Chebyshev Method with parameters $(\alpha,\beta)$ for a \emph{fixed problem} corresponding to the Marchenko-Pastur distribution $(\tau=\tfrac12,\xi=-\tfrac12)$. The 
    color represents the exponent $a$ of the average-case rate $O(t^{a})$ of the method for different values of $\alpha$ and $\beta$. The white star represents the optimal tuning and the blue area is the set of parameters for which the method converges.Note we have a large of region  that guarantee the same optimal asymptotic rate}
    % \end{small}
    \vspace{-1.4cm}
\end{wrapfigure}


For the function value ($l=1$), we find rates that approach $t^{-2}$ as $\xi\rightarrow -1$, showing the worst-case as a limit (over the considered distribution) on the average case.
\begin{comment}


\begin{remark}
We can contextualize our results on the field of orthogonal polynomials as asymptotics on the values of the \textit{Christoffel Functions} at $0$ \citep{totik2005orthogonal}:

\begin{equation}
    \lambda_t(\mu,x)=\inf_{P_t(x):=1,\deg(P_t)\leq t}\int P_t^2 d\mu=\Big( \sum_{k=0}^tp_k(x;\mu)^2\Big)^{-1}
\end{equation}
 For the equivalence classes $\nu_{\tau,\xi}$
\end{remark}
\end{comment}
We remark that the above theorems imply that, at least asymptotically, the GCM  is robust for a suboptimal choice of parameter $\beta$ up to $1/2$ below the optimal choice and infinitely above. 


For completeness, we also derive worst-case rates for the GCM:
\vspace{1.5 cm}

\begin{restatable}{prop}{worstcase}
Let $f$ be a convex, L-smooth quadratic function. Then, for the Generalized Chebyshev Method with parameters $(\alpha,\beta)$, we have
\begin{align}
 f(\xx_t)-f(\xx^\star) \leq C_1L\left\{
    \begin{array}{cc}
           t^{2(\alpha-\beta)} &\text{if} \hspace{0.5 cm} \alpha>\beta-1 \\
         t^{-1-2\beta}, \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha\leq \beta-1\hspace{0.5 cm} \beta\leq \frac{1}{2}\\
         t^{-2}, \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha\leq\beta-1\hspace{0.5 cm} \beta\geq \frac{1}{2} 
    \end{array}
    \right . ,\\
    ||\nabla f(\xx_t)-f(\xx^\star)||\leq C_2L^2\left\{
    \begin{array}{cc}
           t^{2(\alpha-\beta)} &\text{if} \hspace{0.5 cm} \alpha>\beta-2 \\
         t^{-1-2\beta}, \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha\leq \beta-2\hspace{0.5 cm} \beta\leq 3/2\\
         t^{-4}, \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha\leq\beta-2\hspace{0.5 cm} \beta\geq 3/2
    \end{array}.
    \right. 
\end{align}
\end{restatable}


For  a reasonable choice of $\alpha,\beta$, i.e. $\beta\geq \frac{1}{2}$, $\alpha\leq \beta-1$. the function value achieves the theoretical lower bound of $t^{-2}$.

We now analyse the convergence of the Nesterov method. \cite{nesterov2003introductory} has shown that it matches up to a a constant factor a lower bound on the worst-case complexity of non strongly convex problems. A natural question is if this performance would translate to good average case rates. To do so, we will extend \citet{paquette2020halting} proof for the Nesterov method rates under the MP distribution.  
\begin{restatable}{theorem}{nesterovrates} \label{the: neste rates}
Let $\nu$ as in Assumption \ref{assumption}. Then for the Nesterov method
\begin{equation}
    \mathbb{E}[f(\xx_t)-f(\xx^\star)]\sim C'_{1,\nu}
    \Big\{\begin{array}{ll}
		  t^{-2(\xi+2)}& \mbox{if } 
		  \xi<-1/2\\
		  t^{-3}\log t& \mbox{if } 
		  \xi=-1/2\\
		  t^{-(\xi+7/2)}& \mbox{if } 
		  \xi>-1/2
	\end{array}, \qquad 
	\mathbb{E}[||\nabla f(\xx_t)||^2_2] \sim C'_{2,\nu}
		  t^{-(\xi+9/2)}.
\end{equation}
\end{restatable}

The difference between the assymp. rates of Nesterov and the optimal ones are $t^{\xi+l-1/2}$, when $\xi+l>1/2$, $\log t$ when $\xi+l=1/2$ and $0$ otherwise.
This shows that Nesterov is almost optimal when the concentrations near $0$ are relatively high, i.e. low $\xi$.

\begin{restatable}{theorem}{gdrates} \label{the: gd rates}
Let $\nu$ as in Assumption \ref{assumption}. Then for gradient descent
\begin{equation}
\mathbb{E}[f(\xx_t)-f(\xx^\star)]=\Theta(t^{-(\xi+2)}), \hspace{1 cm}
	\mathbb{E}[||\nabla f(\xx_t)||^2_2] =\Theta(t^{-(\xi+3)}).
\end{equation}

\end{restatable}

Observe for the function value that the rate for Nesterov is $t^{-2}$ and the rate for Gradient Descent is $t^{-1}$ when $\xi \rightarrow -1$. 
%we  find the $t^{-2}$ rates for Nesterov and $t^{-1}$ for Gradient Descent  when $\xi\rightarrow-1$

\begin{table}[t]
    \centering
    \begin{tabular}{c|c|c|c|c}
         $(\tau,\xi)$/Method& Chebyshev ($\frac{1}{2},\frac{5}{2}$) & Chebyshev ($\frac{1}{2},\frac{3}{2}$) &  Nesterov  & G.D. \\
         \hline
         ($\frac{1}{2},\frac{1}{2}$)&$t^{-5}$ & $t^{-4}$ & $t^{-4}$ & $t^{-\frac{5}{2}}$\\
         \hline
         ($\frac{1}{2},\frac{-1}{2}$)&$t^{-3}$ & $t^{-3}$ & $t^{-3}\log t$ & $t^{-\frac{3}{2}}$
    \end{tabular}
    \caption{Comparison of asymptotic rates for the function-value for different methods an $(\tau,\xi)$ values}
    \label{tab:theoretic rates}
\end{table}

Lastly, we consider the optimal rates for a Gamma distribution.

\begin{restatable}{theorem}{laguerrerates} \label{thm:laguerrerates}
Let $\alpha>-1$ and $\mu_\alpha$ be a Gamma distribution, i.e. $d\mu_\alpha(x)=\frac{x^\alpha e^{-x}}{\Gamma(\alpha+1)}$. The optimal rates are given by the Laguerre method of appropriate tuning and
\begin{equation}
    \mathbb{E}[f(\xx_t)-f(\xx^\star)]=\Theta(t^{-(\alpha+2)}) . 
    %%we can get the constants here
\end{equation}
\end{restatable}
Note that this result does not have the same universality of the others because of the non-compacity of the distribution's support.

These rates are contrasted  to the worst-case lower bound on the optimization of non-smooth functions by first-order methods, which gives
\begin{equation*}
    f(\xx_k)-f(\xx^\star)\geq\frac{C}{\sqrt{t}} .
\end{equation*}
These rates are not found when $\alpha\rightarrow-1$, indicating that the worst-case is especially pessimistic in this scenario.


\section{Experiments}
\begin{figure}[t!]
    \centering
    \includegraphics[width=6 cm]{final_imgs/cifar_spectrum.pdf}\hspace{0.5 cm}\includegraphics[width= 6 cm]{final_imgs/mnist_spectrum.pdf}
    \includegraphics[width=6 cm]{final_imgs/cifar.pdf}\hspace{0.5 cm} \includegraphics[width= 6 cm]{final_imgs/mnist.pdf}
    \caption{\textit{Above:} Empirical spectrum for the covariance matrix of the features. \textit{Below:} Gradient norms throughout iterations. \textit{Left:} CIFAR-10 Inception features \textit{Right}: MNIST features. Here we choose to compare gradient norms as the minimum function value is not known. The properly tuned GCM achieves remarkable performance under these non-synthetic spectrum's.
}
    \label{fig: real data}
\end{figure}


We simulate the e.s.d's  in two ways. The Marchenko Pastur distribution, which we sample by taking $\HH=\XX\XX^T$ where $\XX$ has i.i.d. gaussian samples. This enables us to simulate $(\tau,\xi)$ values of $(1/2,-1/2)$ 

Other values of $(\tau,\xi)$ are simulated by sampling $\Lambda \in \RR^d$ from the corresponding Beta distribution and taking $\HH=\UU\diag(\Lambda)\UU^T$, where $\UU$ is an independently sampled orthonormal matrix. 

We let $\xx^\star=0$ and sample $\xx_0$ from a centered gaussian distribution, the dynamics are the same as in the general case. In all experiments we use the problem's instance largest eigenvalue to calibrate each method, i.e. the step size of Gradient Descent is $1/L$


\begin{figure}[h]
    \centering
    \includegraphics[width=6 cm]{final_imgs/mp_f.pdf}\includegraphics[width= 6 cm]{final_imgs/mp_grad.pdf}
    
    
    \caption{Rates for a synthetic problem, simulating the Marchenko Pastur distribution. Note that both tunings of the GCM achieve performance in function value very close to the one of Conjugate Gradient, which is optimal for every draw of the problem.}
    \label{fig: mp performance}
\end{figure}




Our theoretical rates in Theorem~\ref{the: gd rates} and Theorem~\ref{the: neste rates} respectively for the Nesterov method and Gradient Descent are precise under the approximate range $-1<\xi<0$  as we show in Figure~\ref{fig: last figure}. Distributions with higher $\xi$ need many samples otherwise they behave as strongly convex functions.



The same is not true for the Generalized Chebyshev Method. If $\beta<\beta^\star$ or $\xi$ is low the empirical findings diverge from the theoretical. We believe this is due to numerical instability under these regimes as the metrics also have  much larger variance than in the other regimes. We've not been able though to pinpoint the exact source of this supposed instability. This is shown in appendix \ref{appendix: experiments}. 


The GCM with  $\beta>\beta^\star$ perform corresponding to the theory, and it's non-asymptotically very close to the performance of $\beta^\star$. High values of $\beta$ also perform very well on non-synthetic data, suggesting in practice we should use these values.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{final_imgs/nesterov.pdf}
    \includegraphics[width= 0.45\textwidth]{final_imgs/gd.pdf}\\
        \includegraphics[width= 0.4\textwidth]{final_imgs/chebyshev.pdf}

    \caption{Comparison between experiments run on synthetic Beta distribution and theoretical asymptotic. Y-axis is function value }
    \label{fig: last figure}
\end{figure}


%%Chebyshev \beta=5/2 is not giving the expected gradient performance ...


\section{Conclusion and further work}
In this paper we've established that the assymptotic convergence of first order methods on quadratic problems in the convex regime depend on the concentration of the Hessian's eigenvalue near the edges of the spectrum's support. We further contributed to the theoretic understanding of the Nesterov's method performance and established the contrast between the worst-case and average-case in the main regimes considered in Optimization.

We leave as future work the analysis of the Generalized Chebyshev method on non-quadratic problems. We conjecture its possible to achieve local convergence for general convex functions by a similar method to \cite{wang2021modular}, which has shown the size of the convergent neighborhood for the Polyak method for general strongly convex smooth functions.
\newpage
\bibliography{citations.bib}
\bibliographystyle{ICLR/iclr2022_conference}

\appendix
\newpage
\section{Proofs of Section \ref{section: average case}}

%%lorem ipsum lorem ipsum
\metrics*
\begin{proof}
\newcommand\xinit{\xx_0-\xx^\star}
We remark that by the definition of the expected spectral distribution $\mu$ of $\HH$, we have for continuous $g$
\begin{equation}
    \EE_H[g(tr(\HH))]=\int g(\lambda)\dif \mu(\lambda) 
\end{equation}
We know that $\xx_t-\xx^\star=P_t(\HH)(\xinit)$. We can write $||\xx_t-\xx^\star||^2$ in terms of a trace and use the independence of $\HH$ and $\xinit$ to connect it to the e.s.d.:
\begin{align}
    \mathbb{E}||\xx_t-\xx^\star||^2&=\EE[ tr((\xinit)^TP_t(\HH)^2(\xinit))] \\
    &=\EE_{\HH,\xinit} [tr(P_t(\HH)^2(\xinit)(\xinit)^T]\\
    &=\EE_\HH\left[P_t(\HH)^2\EE_{\xinit}[(\xinit)(\xinit)^T])\right]  \\
    &=R^2\EE_\HH[P_t(tr(\HH))^2]=R^2 \int P_t(\lambda)^2\dif\mu(\lambda)
\end{align}
For the gradient and function value the reasoning is the same by noticing that
\begin{align}
    \EE[f(\xx_t)-f(\xx^\star)]&=\EE[ tr((\xinit)^TP_t(\HH)\HH P_t(\HH)(\xinit))]\\
    &=\EE_\HH[(\lambda P_t)(\tr(H))^2],
\end{align}
where $\lambda P_t$ is also a  polynomial. As $\nabla f(\xx_t)=\HH(\xx_t-\xx^\star)$.
\begin{align}
    \EE||\nabla f(\xx_t))||^2&=\EE[ tr((\xinit)^TP_t(\HH)\HH^2 P_t(\HH)(\xinit))]\\
    &=\EE_\HH[(\lambda^2 P_t)(\tr(H))^2]
\end{align}


\end{proof}

\optimality*
\begin{proof}
We differentiate the expression for the metrics w.r.t. to the coefficients of the polynomials:
\begin{align*}
    \frac{d}{da_k}\left(\int \lambda^lP_t^2(\lambda)d\mu(\lambda)\right)&=\int\lambda^l\frac{\dif}{\dif a_k}\left(\sum_{k=0}^t a_k\lambda^kP_t(\lambda) \right) \dif \mu(\lambda)=
    \\&=2\cdot\left(\int \lambda^{l+k}P_t(\lambda)d\mu(\lambda)\right)=0
\end{align*}
This means that $P_t(\lambda)$ is orthogonal to any polynomial of degree $t-1$  w.r.t to the intern product $\langle.,.\rangle_{\lambda^{l+1}d\mu}$

\end{proof}

\section{GCM and Laguerre method derivation}
We will first state two lemmas that allow us to construct the optimal polynomials. With them in hand the procedure is trivial.

\begin{lemma} \label{lemma: to residual}
Let $(\tp_t)$ be a family polynomials  following  
\begin{equation*}
    \tp_t(\lambda)=(\alpha_t+\beta_t\lambda)\tp_{t-1})\lambda)+\gamma_t\tp_{t-2}(\lambda),
\end{equation*}
with $\tp_0$ a constant polynomial and $\tp_t\neq0, \forall t$. Then
\begin{equation}
    P_t(\lambda)=(a_t+b_t\lambda)P_{t-1}(\lambda)+(1-a_t)P_{t-2}(\lambda)
\end{equation} is the recurrence for $P_t(\lambda)=\tp_t(\lambda)/\tp_t(0)$. With:
\begin{align}
    a_t&=\delta_t\alpha_t \\
    b_t&=\delta_t\beta_t \\
    \delta_t&=(\alpha_t+\gamma_t\delta_{t-1}) \hspace{0.5 cm } (\delta_0=0)
\end{align}
\end{lemma}
The proof of this is presented in \cite{pedregosa2020acceleration}. Further, we know how to compute the recurrence for the polynomials of a shifted distribution:
\begin{lemma}

Let $(\tp_t)$ be a family polynomials orthogonal w.r.t  following  
\begin{equation}    \label{rec}
    \tp_t(\lambda)=(\alpha_t+\beta_t\lambda)\tp_{t-1}(\lambda)+\gamma_t\tp_{t-2}(\lambda) ,
\end{equation}
and define polynomials $P_t$ s.t. :
$$
P_t(m(\lambda))=\tp_t(\lambda),
$$
with $m(\lambda)=a\lambda+b$ a non singular affine transform. Then $P_t$ follows a recurrence like in eq. \eqref{rec}, with:
\begin{align}
    \alpha_t'&=\alpha_t+b\beta_t \\
    \beta'_t&=a\beta_t\\
    \gamma'_t&=\gamma_t
\end{align}
\end{lemma}
The lemma is self-evident by considering eq. \eqref{rec} with argument $m^{-1}(\lambda)$


\label{jacobi recurrence}
These results are enough to  get  the recurrence relation for the residual polynomial w.r.t $x^\beta(L-x)^\alpha$. We begin by the standard jacobi polynomials, which are orthogonal w.r.t $(1-x)^\alpha(1+x)^\beta$ and follow a recurrence according to $\alpha_t,\beta_t,\gamma_t$ below, shift the distribution according to $\eta(x)$, and then transform to the residual ones. We slightly simplify these computations and use remark \ref{rmk: momentum based} to get Algorithm \ref{algo: chebyshev}. \\

We know \citep{szego1975orthogonal} that the Laguerre polynomials $L_t^\alpha$, with usual normalization, follow the recurrence

\begin{equation}
    L_t^\alpha(\lambda)=\left(\frac{2t+\alpha-1}{t}-\frac{1}{t}\lambda\right)L_{t-1}^\alpha(\lambda)+\frac{t+\alpha-1}{t}L_{t-2}^\alpha(\lambda)
\end{equation}
As we don't have to shift the domains, we have only to apply lemma \ref{lemma: to residual} to get the Laguerre method.Further, we can get a explicit expression for $\delta_t=\frac{t}{t+\alpha}$, simplifying the expression.

\begin{algorithm}
\caption{Laguerre($\alpha$)}
\textbf{Inputs}: Initial vector $\xx_0$, function $f$ \\
$\xx_{-1}\gets \textbf{0}$\\
\For{$t=1,\ldots,T$}{
$\xx_t\gets\xx_{t-1}+\frac{t-1}{t+\alpha}(\xx_{t-1}-\xx_{t-2})-\frac{1}{t+\alpha}\nabla f(\xx_{t-1})$
} 
\end{algorithm}

\section{Proofs of section 3}
In the following we will consider shifted versions of the spectral distributions. This shift is written as an affine transform $m(\lambda):[0,L]\rightarrow [-1,1]$ because most results in the theory of orthogonal polynomials are stated in terms of distributions supported in $[-1,1]$. \\
This can be seen as an additional layer of abstraction because the quantities evaluated with the shifted distributions and polynomials are proportional, i.e. if $P_t(x)=\tp_t(m(x))$ and $\mu'(x)=\tmu'(m(x))$:
\begin{equation}
    \int P_t^2(x)\mu'(x)\dif x\propto \int \tp_t^2(x)\tmu'(x)\dif x
\end{equation}
So all the asymptotics are the same. The Jacobi polynomials $\JP_t$ are those orthogonal w.r.t $d\mu(x)=(1-x)^\alpha(1+x)^\beta$. Most works use the  normalization $\tp_t^{\alpha,\beta}(-1)=(-1)^t\binom{t+\beta}{t}$. We will write $\tp^{\alpha,\beta}_t$ for this normalization and $\JP_t$ for the residual polynomials
\robustjacobi*
\begin{proof}
We will prove that for any $\alpha$ and $\beta$, $\xi,\tau>-1$, $l>0$ and $\nu$ following Assumption \ref{assumption}, we have
\begin{equation*}
    \int P_t^{\alpha,\beta}(x)^2x^l d\nu_{\tau,\xi-l}(x) \sim  L^lC^{\alpha,\beta}_\nu\left\{
	\begin{array}{ll}
		  t^{-1-2\beta}& \mbox{if } 
		  \alpha<\tau+1/2 \text{ and } \beta <\xi+1/2\\
		  t^{-2(\xi+1)}\log t& \mbox{if } 
		  \alpha=\tau+1/2 \text{ and } \beta =\xi+1/2\\
		  t^{2(\max\{\alpha-\beta-\tau,-\xi\}-1)}& \mbox{if } 
		  \alpha>\tau+1/2 \text{ or } \beta >\xi+1/2
	\end{array}
\right.
\end{equation*}
We will first show this result for the Beta weights, then show that distributions with the same concentration behave similarly. \\
The normalization of $\tp^{\alpha,\beta}_t$ is s.t.[ \cite{szego1975orthogonal} (4.3.3)]:
\begin{equation}
    \int_{-1}^1\tp_t^{\alpha,\beta}(x)(1-x)^\alpha(1+x)^\beta \dif x=\frac{2^{\alpha+\beta+1}}{2n+\alpha+\beta+1}\frac{\Gamma(n+\alpha+1)\Gamma(n+\beta+1)}{\Gamma(n+1)\Gamma(n+\alpha+\beta+1)}=\Theta(t^{-1})
\end{equation}

Further, the residual polynomials  are s.t. $|P^{\alpha,\beta}_t|=\Theta(t^{-\beta})|\tp^{\alpha,\beta}_t|$, from the definition of the classical normalization.\\
We state the result (Exercise 91, Generalisation of 7.34.1) from \cite{szego1975orthogonal}:
\begin{lemma}
We have
\begin{align}
    &\int_0^1(1-x)^\tau P_t^{\alpha,\beta}(x)^2dx \sim\Theta( h_{\tau}^\alpha) \\
    &h_{\tau}^\alpha:=
\left\{
	\begin{array}{ll}
		t^{2(\alpha-\tau-1)}  & \mbox{if } \alpha>\tau+1/2 \\
		t^{-1}\log t   & \mbox{if } \alpha=\tau+1/2 \\
		t^{-1}   & \mbox{if } \alpha<\tau+1/2
	\end{array}
\right.
\end{align}
 \label{jacobi lemma}
\end{lemma}
Noting that $\tp^{\alpha,\beta}_t(x)=(-1)^t\tp_t^{\beta,\alpha}(-x)$, we can write:
\begin{equation}
    \int_{-1}^1\tp_t(x)^2(1-x)^\tau(1+x)^\xi dx = \Theta\left(\int_0^1(1-x)^\tau|\tp_t^{\alpha,\beta}(x)|^2dx\right) +\Theta\left(\int_0^1(1-x)^\xi|\tp_t^{\beta,\alpha}(x)|^2dx\right) \label{int decomp}
\end{equation}
We can then show our result for $\dif\nu_{\tau,\xi-l}(x)=x^{\xi-l}(L-x)^\alpha$ by carefully considering each of the cases on Lemma~\ref{jacobi lemma} and the maximum of each term in eq. \ref{int decomp}, and an added $t^{-2\beta}$ from the different normalization. With this, we have the wanted result for the Beta weights \\
It remains to show:
\begin{equation}
    \int_0^1 \tp_t^{\alpha,\beta}(x)^2\dif\nu_{\tau,\xi}(x)= \Theta\left(\int_0^1(1-x)^\tau\tp_t^{\alpha,\beta}(x)^2dx \right)
\end{equation}
And the rest follows from the same arguments. We do this with the help of this lemma shown in \cite{van1995weak} relating to the weak convergence of the orthogonal polynomials:

\begin{lemma}
 Let $\mu$ be a measure and $(p_t)$ it`s family of orthonormal polynomials s.t.  $p_t$ follow the recurrence:
 \begin{equation*}
     xp_t(x)=a_tp_{t+1}(x)+b_tp_t(x)+a_{t-1}p_{t-1}(x)
 \end{equation*}
 and $a_t,b_t$ converge respectively to $a,b$. Then for any $f$  continuous and bounded:
\begin{equation}
    \int f(x)p_t^2(x)d\mu(x) \rightarrow \frac{1}{\pi}\int_{-1}^1 \frac{f(x)}{\sqrt{1-x^2}}dx   
\end{equation}
\label{wk}
\end{lemma} 
 Let $\epsilon$ s.t.
\begin{equation}
    x\geq 1-\epsilon \Rightarrow |\dif\nu_{\tau,\xi}-A(1-x)^\tau|\leq  B(1-x)^\tau \label{eq: epsilon}
\end{equation}
We observe that for $0<x<1-\epsilon$, $f(x)=\frac{d\nu_{\tau,\xi}}{(1-x)^\alpha(1+x)^\beta}$ is bounded. \\
We get from an application of \ref{wk}, and the observation that $\tp_t^{\alpha,\beta}=\mathcal{N}_tp_t^{\alpha,\beta}$, with $\mathcal{N}_t=\Theta(t^{-1/2})$:
\begin{align}
    \underbrace{\int_0^1(1-x)^\tau\tp_t^{\alpha,\beta}(x)^2\dif x}_{\Theta( h_\tau^\alpha)}&=\underbrace{\int_0^{1-\epsilon}(1-x)^\tau\tp_t^{\alpha,\beta}(x)^2\dif x}_{\Theta( t^{-1})
    } +\int_{1-\epsilon}^1(1-x)^\tau\tp_t^{\alpha,\beta}(x)^2\dif x \Rightarrow\\
    &\int_{1-\epsilon}^1(1-x)^\tau\tp_t^{\alpha,\beta}(x)^2\dif x  =\Theta( h_\tau^\alpha) 
    \end{align}
\begin{equation}
    \begin{split}
    \int_0^1 \tp_t^{\alpha,\beta}(x)^2\dif\nu_{\tau,\xi}(x)&=
    \underbrace{\int_0^{1-\epsilon} \tp_t^{\alpha,\beta}(x)^2 f(x) (1-x)^\alpha(1+x)^\beta\dif x)}_{\Theta( t^{-1})
    }\\
    &+\Theta\left(
    \underbrace{\int_{1-\epsilon}^1(1-x)^\tau\tp_t^{\alpha,\beta}(x)^2\dif x}_{\Theta( h_\tau^\alpha)}\right)
    \end{split}
\end{equation}


\end{proof}

\jacoptimal*
\begin{proof}
We will prove that for $\tau,\xi>-1$ If $\alpha = \tau$ and $\beta = \xi+l+1$ (i.e., are optimal), the rate of convergence reads
\begin{equation}
    \min_{P_t(0)=1}\int P_t^2(\lambda)\lambda^ld\nu(\lambda)=\Theta\left( \int_{0}^l  \tp_t^{\alpha,\beta}(\lambda)^2(L-\lambda)^\tau\lambda^{\xi+l}\dif \lambda\right) =\Theta( t^{-2(\xi+l+1)})
\end{equation}
Showing the second equality is easy by considering theorem \ref{thm: jacobirates}, and that is further the minimum asymptotic rate for the Beta distribution. \\
As $\tp^{\alpha,\beta}_t$ has the same rate on $\nu$ as  on the Beta distribution , the minimum rate for $\nu$ is lower bounded by the r.h.s. \\
We argue that, setting $P^\nu_t=\frac{p^\nu}{p^\nu_t(0)}$ the optimal residual and orthonormal polynomials and $\mu_{\tau,\xi}$ the Beta distribution  w.r.t, $P^\nu_t$ must have the same rate on $\nu$ as it does on $\nu$. Indeed, setting $\epsilon_1,\epsilon_2$ as in eq. \ref{eq: epsilon}, we argue:
\begin{align}
    \int_{1-\epsilon_2}^{1}P_t^\nu(x)^2d\nu(x)&=\left(\Theta\int_{1-\epsilon_2}^{1}P_t^\nu(x)^2d\mu(x)\right)\\
    \int_{-1}^{-1+\epsilon_1}P_t^\nu(x)^2d\nu(x)&=\Theta\left(\int_{-1}^{-1+\epsilon_1}P_t^\nu(x)^2d\mu(x)\right)\\
    \int_{-1+\epsilon_1}^{1-\epsilon_2}P_t^\nu(x)^2d\nu(x)&=\Theta\left(\int_{-1+\epsilon_1}^{1-\epsilon_2}P_t^\nu(x)^2d\mu(x)\right)=\Theta\left(\frac{1}{p_t^\nu(-1)^2}\right)\\
\end{align}
Where the first two equations come from the fact that $\nu=\Theta(\mu)$ near $-1$ and $1$ and the third from lemma \ref{wk}.\\
This effectively upper bounds the rates on $\nu$ because the rates of $P_t^\nu$ on $\mu_{\tau,\xi}$ can't be lower than $-2(\xi+1)$.
\end{proof}




\worstcase *
\begin{proof}
rates]
We will prove that:  $\sup_{x\in[0,L]}x^lP_t^{\alpha,\beta}(x)^2=O(L^lt^{v(\alpha,\beta,l)})$. Where:
\begin{equation}
    v(\alpha,\beta,l)=\left\{
    \begin{array}{cc}
           2(\alpha-\beta) &\text{if} \hspace{0.5 cm} \alpha>\beta-l \\
         -1-2\beta, \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha\leq \beta-l\hspace{0.5 cm} \beta\leq l-\frac{1}{2}\\
         -2l, \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha\leq\beta-l\hspace{0.5 cm} \beta\geq l-\frac{1}{2} 
    \end{array}
    \right . 
\end{equation}
From \cite{szego1975orthogonal}, Theorem 7.32.2, if $\theta<\frac{\pi}{2}$:
\begin{equation}
    \tp_t^{\alpha,\beta}(\cos \theta)=\left\{ 
    \begin{array}{cc}
         O(t^{-1/2})  \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha < -\frac{1}{2} \\
         O(t^{\alpha})  \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha \geq -\frac{1}{2} , 0\leq\theta\leq ct^{-1} \\
         \theta^{-
         \alpha-1/2}O(t^{-1/2})  \hspace{1 cm} &\text{if} \hspace{0.5 cm} \alpha \geq -\frac{1}{2} , \theta> ct^{-1}
    \end{array}
    \right .
\label{lemma: worst case}
\end{equation}
We observe that, from the symmetry of the jacobi polynomials:
\begin{equation}
    \sup_{x\in[0,L]}x^lP_t^{\alpha,\beta}(x)^2 =\Theta\left( \max\left\{\sup_{x\in[0,1]}P_t^{\alpha,\beta}(x)^2,\sup_{x\in[0,1]}(1-x)^lP_t^{\beta,\alpha}(x)^2\right\}\right)
\end{equation}
The $(1-x)^l$ term,  corresponds to $(2\sin(\frac{\theta}{2}))^l$ in the variable $\theta$, which is $O(\theta^{2l})$. The rest follows from carefully considering the expressions given by eq. \ref{lemma: worst case}.
\end{proof}


\nesterovrates * 


\begin{proof}
We will prove:
\begin{equation}
    \int_0^1P_t^{\text{Nes}}(\lambda)^2\lambda^l\dif\nu_{\tau,\xi-l}\sim C'_\nu
    \Big\{\begin{array}{ll}
          t^{-2(\xi+1)}& \mbox{if } 
		  0<\xi<1/2  \\
		  t^{-3}\log t& \mbox{if } 
		  \xi=1/2\\
		  t^{-(\xi+5/2)}& \mbox{if } 
		  \xi>1/2
	\end{array}
\end{equation}
\cite{paquette2020halting} has shown that the nesterov polynomials $P_t$ are asymptotically, in $t$:
\begin{equation}
    P_t(\lambda)\sim\frac{2J_1(t\sqrt{\alpha\lambda})}{t\sqrt{\alpha\lambda}}e^{-\alpha\lambda t/2}
\end{equation}
In the sense that:
\begin{equation}
    \int_0^1u^{l}\left[\Tilde{P_t^2(u)}-\frac{4J_1^2(t\sqrt{u})}{t^2u}e^{-u t}\right]4\dif\mu_{MP}(u)=O(t^{-(l+25/12))}
\end{equation}
The arguments can be easily used to show that such an integral is $O(t^{ -(\alpha+l+31/12)})$ when evaluated wrt a general $\dif\mu$ s.t $\mu'=\Theta(\lambda^\alpha)$ near $0$. \\
We can thus consider our integral  of interest substituting $P_t^\text{Nes}$ by it's Bessel asymptotic and dividing it into three regions, i.e. $[0,1]=[0,\frac{\epsilon}{t}]\cup[\frac{\epsilon}{t},\frac{\epsilon}{\sqrt{t}}]\cup[\frac{\epsilon}{\sqrt{t}},1]$ corresponding to two different regimes for the Bessel function. The first region will give us the asymptotic and the others we will bound.\\
We consider first, for some $\epsilon>0$:
\begin{equation}
    \int_{\frac{\epsilon}{t}}^{\frac{\epsilon}{\sqrt{t}}} u^{\xi}\frac{4J_1^2(t\sqrt{u})}{t^2u}e^{-u t}\dif u
\end{equation}
We note the asymptotic for $J_1^2$:
\begin{equation}
    J_1^2(\sqrt{tv}) \sim \frac{1}{\pi\sqrt{tv}}(1+\cos(2\sqrt{tv}+2\gamma))
\end{equation}
Doing the change of variable $v=tu$, and identifying the upper limit of the interval, which is $\epsilon t^{1/2}$ to $\infty$:

\begin{align}
    \int_{\frac{\epsilon}{t}}^{\frac{\epsilon}{\sqrt{t}}} u^{\xi}\frac{4J_1^2(t\sqrt{u})}{t^2u}e^{-u t}\dif u &=\Theta\left(
    t^{-2-\xi}\int_\epsilon^\infty v^{\xi-1}J_1^2(\sqrt{tv})e^{-v}\dif v\right)\\
    &=\Theta\left( t^{-2-\xi}\int_\epsilon^\infty v^{\xi-1}\frac{1}{\pi\sqrt{tv}}e^{-v}\dif v \right)\\
    &=\Theta\left(t^{-\frac{5}{2}-\xi}\underbrace{\int_\epsilon^\infty v^{\xi-\frac{3}{2}}\frac{1}{\pi\sqrt{tv}}e^{-v}\dif v}_{\Gamma(\xi-\frac{1}{2},\epsilon) }\right)
\end{align}
Where the cosinus term goes to $0$ from the Riemann-Lebesgue lemma and $\Gamma$ is the incomplete Gamma function.\\
The term corresponding to the interval $[\epsilon t^{-1/2},1]$ is exponentially small. Indeed, because of the exponential $e^{-ut}$ it is  $O(e^{-\epsilon\sqrt{t}})$. This shows that the integral concentrates in a region that is closer and closer to $0$ and that only the behaviour of the distribution near $0$ matters.\\
We have for the $[0,\frac{\epsilon}{t}]$ region, doing the change of variables $v=t^2u$:
\begin{equation}
    \int_0^{\frac{\epsilon}{t}} u^{\xi}\frac{4J_1^2(t\sqrt{u})}{t^2u}e^{-u t}\dif u =\Theta\left(
    t^{-2(\xi+1)}\int_0^{t\epsilon} v^{\xi}\frac{J_1^2(\sqrt{v})}{v}e^{-\frac{v}{t}}\dif v\right)
\end{equation}
And the $e^{\frac{-v}{t}}$ is $\Theta(1)$. We have the following Bessel asymptotics:
\begin{align}
    \frac{J_1^2(\sqrt{v})}{v}&\sim \frac{1}{4}, \hspace{2 cm} v\rightarrow 0 \\
    \frac{J_1^2(\sqrt{v})}{v}&= O(v^{-3/2}), \hspace{1.0 cm} v\rightarrow \infty
\end{align}
So we divide this integral aswell:

\begin{align}
    t^{-2(\xi+1)}\int_1^{t\epsilon} v^{\xi}\frac{J_1^2(\sqrt{v})}{v}e^{-\frac{v}{t}}\dif v
    &=\Theta\left(t^{-2(\xi+1)}\int_\epsilon^{t\epsilon} v^{\xi-\frac{3}{2}}\dif v\right) =\Theta\left( I_\xi(t)t^{-\xi-\frac{5}{2}}\right)\\
    t^{-2(\xi+1)}\int_0^{1} v^{\xi}\frac{J_1^2(\sqrt{v})}{v}e^{-\frac{v}{t}}\dif v
    &=\Theta\left(t^{-2(\xi+1)}\int_0\epsilon^{1} v^{\xi} \dif v\right) =\Theta\left( t^{-2(\xi+1)}\right)
\end{align}
Where $I_\xi(t)=\log t$ if $\xi=\frac{1}{2}$ and $1$ otherwise. \\
The nesterov rate is then $I_\xi(t)t^{-\xi-\frac{5}{2}}$ if $\xi\geq\frac{1}{2}$ and $t^{-2(\xi+1)}$ if $0<\xi<\frac{1}{2}$
\end{proof}
\gdrates*
\begin{proof}
Considering that $P_t^\text{GD}(\lambda)=(1-\frac{\lambda}{L})^t$ we will prove :
\begin{equation}
    \int_0^1(1-\lambda)^{2t}\lambda^l\dif\nu_{\tau,\xi-l}=\Theta(t^{-(\xi+l+1)}
\end{equation}
We know, for the Beta weights, that:
\begin{equation}
    \int_0^1(1-\lambda)^{2t+\tau}\lambda^{\xi+l}\dif\lambda=\frac{\Gamma(l+\xi+1)\Gamma(2t+\tau+1)}{\Gamma(2t+l+\xi+\tau+2)}=\Theta(t^{-(\xi+l+1)})
\end{equation}
We can indentify this asymptotic to the interval $\int_0^\epsilon$ for any $\epsilon$ because:
\begin{equation}
    \int_\epsilon^1(1-\lambda)^{2t+\tau}\lambda^{\xi+l}\dif\lambda=\mathcal{O}((1-\epsilon)^{2t})
\end{equation}
Then:
\begin{align}
    \int_\epsilon^1(1-\lambda)^{2t}\lambda^l\dif\nu_{\tau,\xi-l}&=\mathcal{O}((1-\epsilon)^{2t}) \\
    \int0^\epsilon(1-\lambda)^{2t}\lambda^l\dif\nu_{\tau,\xi-l}&=\Theta\left(\int_0^\epsilon(1-\lambda)^{2t+\tau}\lambda^{\xi+l}\dif\lambda\right)=\Theta(t^{-(\xi+l+1)})
\end{align}

\end{proof}
\laguerrerates*
\begin{proof}
Let $L_t^\alpha$ be the Laguerre polynomials with the usual normalization \cite{szego1975orthogonal}:
\begin{equation}
    \int L_t^\alpha(x)^2d\mu_\alpha(x)=L_t^\alpha(0)=\binom{n+\alpha}{n} 
\end{equation}
We further now [\cite{szego1975orthogonal} (5.1.13)]]:
\begin{equation}
    \sum_{k=0}^tL_t^\alpha(x)=L_t^{\alpha+1}(x)
\end{equation}
Thus, letting $P_t^\alpha$ be the residual laguerre polynomial, we consider:
\begin{equation}
\begin{split}
    \EE[f(\xx_t)-f(\xx^\star)]&=\int P_t^{\alpha+2}(\lambda)^2d\mu_{\alpha+1}(\lambda)=\binom{t+\alpha+2}{t}^{-2}\int L_t^{\alpha+2}d\mu_{\alpha+1}(\lambda)\\
    &=\binom{t+\alpha+2}{t}^{-2}\sum_{k=0}^{t}\left[\int L_k^{\alpha+1}(\lambda)d\mu_{\alpha+1}(\lambda)\right]\\
    &=\binom{t+\alpha+2}{t}^{-2}\sum_{k=0}^t\binom{k+\alpha+1}{k}=\binom{t+\alpha+2}{t}^{-2}\binom{t+\alpha+2}{t}\\
    &=\binom{t+\alpha+2}{t}^{-1}=\Theta( t^{-(\alpha+2)})
\end{split}
\end{equation}
\end{proof}
\section{Additional Experiments} \label{appendix: experiments}

\begin{figure}[H]
    \centering
    \includegraphics[width= 0.4 \textwidth]{new_imgs/stable xi.png}
    \includegraphics[width= 0.4 \textwidth]{new_imgs/unstable xi.png}
    \caption{Empirical vs Theoretical function-value performance for $\text{GCM}(\alpha^\star,\beta^\star)$ . Red lines are given by numerical integration, shades are minimum and maximum values under 10 runs}
    \label{fig:my_label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width= 0.4 \textwidth]{new_imgs/stable beta.png}\includegraphics[width= 0.4 \textwidth]{new_imgs/unstable beta.png}
    \caption{Empirical vs Theoretical function-value performance under Marchenko Pastur distribution. Red lines are given by numerical integration,shades are minimum and maximum values under 10 runs}
    \label{fig:my_label}
\end{figure}
We note that in the regimes where the empirical average performance doesn't match the theoretical one, we can still find samples of problems who do match. This and the much larger variance on the function-value, this discrepancy is due to numerical unstability in these regimes.
\end{document}